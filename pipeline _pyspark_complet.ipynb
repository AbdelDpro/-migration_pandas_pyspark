{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e94ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 0. Racine du projet (attention √† __file__ si tu es en notebook)\n",
    "# -------------------------------------------------------------------\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[2]\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path(\"/mnt/c/Users/alexa/Simplon/Esther/Exos/Starter stack pour Data Engineers - Partie 1\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Chargement de settings.yaml (identique √† ta version)\n",
    "# -------------------------------------------------------------------\n",
    "def load_settings(path: str = \"settings.yaml\") -> dict:\n",
    "    \"\"\"\n",
    "    Charge settings.yaml en basant les chemins relatifs sur la racine du\n",
    "    projet (deux niveaux au-dessus de ce script).\n",
    "    \"\"\"\n",
    "    base_dir = PROJECT_ROOT\n",
    "    cfg_path = Path(path)\n",
    "    if not cfg_path.is_absolute():\n",
    "        cfg_path = (base_dir / cfg_path).resolve()\n",
    "    if not cfg_path.exists():\n",
    "        raise FileNotFoundError(f\"Settings file not found: {cfg_path}\")\n",
    "    with cfg_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def resolve_path(value, default: str) -> Path:\n",
    "    \"\"\"\n",
    "    Si 'value' est d√©fini on l'utilise, sinon 'default'.\n",
    "    On convertit en chemin absolu bas√© sur PROJECT_ROOT.\n",
    "    \"\"\"\n",
    "    target = Path(value if value is not None else default)\n",
    "    if not target.is_absolute():\n",
    "        target = (PROJECT_ROOT / target).resolve()\n",
    "    return target\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Cr√©ation de la SparkSession\n",
    "# -------------------------------------------------------------------\n",
    "def create_spark(app_name: str = \"StarterStack_PySpark\"):\n",
    "    \"\"\"\n",
    "    Cr√©e une SparkSession locale.\n",
    "\n",
    "    üêº Pandas : rien √† faire, tu manipules des DataFrame en m√©moire.\n",
    "    üî• PySpark : tu DOIS cr√©er une session Spark pour avoir un contexte distribu√©.\n",
    "    \"\"\"\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .master(\"local[*]\")  # toutes les cores locales\n",
    "        # .config(\"spark.sql.shuffle.partitions\", \"8\")  # optionnel\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return spark\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Lecture CSV avec Spark (remplace pd.read_csv)\n",
    "# -------------------------------------------------------------------\n",
    "def read_csv_spark(spark, path: Path, sep: str, enc: str):\n",
    "    \"\"\"\n",
    "    Lecture CSV fa√ßon Spark.\n",
    "\n",
    "    üêº Pandas : pd.read_csv(path, sep=sep, encoding=enc)\n",
    "    üî• PySpark : spark.read.option(...).csv(...)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"sep\", sep)\n",
    "        .option(\"encoding\", enc)\n",
    "        .csv(str(path))\n",
    "    )\n",
    "    \n",
    "cfg = load_settings()\n",
    "\n",
    "in_dir = resolve_path(cfg.get(\"input_dir\"), \"data/march-input\")\n",
    "out_dir = resolve_path(cfg.get(\"output_dir\"), \"data/out\")\n",
    "db_path = resolve_path(cfg.get(\"db_path\"), \"data/sales_db.db\")\n",
    "\n",
    "sep = cfg.get(\"csv_sep\",\",\")\n",
    "enc = cfg.get(\"csv_encoding\", \"utf-8\")\n",
    "ffmt = cfg.get(\"csv_float_format\", \"%.2f\")  # si tu t'en sers plus tard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d40d913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------------------+---------+---------+\n",
      "|customer_id|first_name|last_name|email             |city     |is_active|\n",
      "+-----------+----------+---------+------------------+---------+---------+\n",
      "|C0001      |User1     |Test1    |user1@example.com |Nantes   |yes      |\n",
      "|C0002      |User2     |Test2    |user2@example.com |Toulouse |yes      |\n",
      "|C0003      |User3     |Test3    |user3@example.com |Bordeaux |y        |\n",
      "|C0004      |User4     |Test4    |user4@example.com |Bordeaux |true     |\n",
      "|C0005      |User5     |Test5    |user5@example.com |Lyon     |true     |\n",
      "|C0006      |User6     |Test6    |user6@example.com |Marseille|false    |\n",
      "|C0007      |User7     |Test7    |user7@example.com |Toulouse |true     |\n",
      "|C0008      |User8     |Test8    |user8@example.com |Marseille|false    |\n",
      "|C0009      |User9     |Test9    |user9@example.com |Toulouse |false    |\n",
      "|C0010      |User10    |Test10   |user10@example.com|Bordeaux |true     |\n",
      "|C0011      |User11    |Test11   |user11@example.com|Lyon     |true     |\n",
      "|C0012      |User12    |Test12   |user12@example.com|Toulouse |f        |\n",
      "|C0013      |User13    |Test13   |user13@example.com|Marseille|false    |\n",
      "|C0014      |User14    |Test14   |user14@example.com|Paris    |true     |\n",
      "|C0015      |User15    |Test15   |user15@example.com|Paris    |true     |\n",
      "|C0016      |User16    |Test16   |user16@example.com|Lille    |true     |\n",
      "|C0017      |User17    |Test17   |user17@example.com|Paris    |true     |\n",
      "|C0018      |User18    |Test18   |user18@example.com|Marseille|true     |\n",
      "|C0019      |User19    |Test19   |user19@example.com|Bordeaux |true     |\n",
      "|C0020      |User20    |Test20   |user20@example.com|Marseille|true     |\n",
      "|C0021      |User21    |Test21   |user21@example.com|Toulouse |true     |\n",
      "|C0022      |User22    |Test22   |user22@example.com|Nice     |true     |\n",
      "|C0023      |User23    |Test23   |user23@example.com|Nice     |true     |\n",
      "|C0024      |User24    |Test24   |user24@example.com|Marseille|true     |\n",
      "|C0025      |User25    |Test25   |user25@example.com|Marseille|false    |\n",
      "|C0026      |User26    |Test26   |user26@example.com|Lille    |true     |\n",
      "|C0027      |User27    |Test27   |user27@example.com|Nantes   |true     |\n",
      "|C0028      |User28    |Test28   |user28@example.com|Lille    |true     |\n",
      "|C0029      |User29    |Test29   |user29@example.com|Bordeaux |true     |\n",
      "|C0030      |User30    |Test30   |user30@example.com|Toulouse |true     |\n",
      "|C0031      |User31    |Test31   |user31@example.com|Nantes   |true     |\n",
      "|C0032      |User32    |Test32   |user32@example.com|Nice     |true     |\n",
      "|C0033      |User33    |Test33   |user33@example.com|Bordeaux |true     |\n",
      "|C0034      |User34    |Test34   |user34@example.com|Nantes   |true     |\n",
      "|C0035      |User35    |Test35   |user35@example.com|Nantes   |true     |\n",
      "|C0036      |User36    |Test36   |user36@example.com|Nice     |false    |\n",
      "|C0037      |User37    |Test37   |user37@example.com|Bordeaux |true     |\n",
      "|C0038      |User38    |Test38   |user38@example.com|Nice     |true     |\n",
      "|C0039      |User39    |Test39   |user39@example.com|Bordeaux |false    |\n",
      "|C0040      |User40    |Test40   |user40@example.com|Lille    |true     |\n",
      "|C0041      |User41    |Test41   |user41@example.com|Nice     |true     |\n",
      "|C0042      |User42    |Test42   |user42@example.com|Toulouse |true     |\n",
      "|C0043      |User43    |Test43   |user43@example.com|Nantes   |false    |\n",
      "|C0044      |User44    |Test44   |user44@example.com|Nice     |true     |\n",
      "|C0045      |User45    |Test45   |user45@example.com|Bordeaux |true     |\n",
      "|C0046      |User46    |Test46   |user46@example.com|Marseille|true     |\n",
      "|C0047      |User47    |Test47   |user47@example.com|Paris    |true     |\n",
      "|C0048      |User48    |Test48   |user48@example.com|Toulouse |true     |\n",
      "|C0049      |User49    |Test49   |user49@example.com|Nice     |false    |\n",
      "|C0050      |User50    |Test50   |user50@example.com|Nice     |true     |\n",
      "+-----------+----------+---------+------------------+---------+---------+\n",
      "only showing top 50 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Taille: (800, 6)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = create_spark()\n",
    "# On reste coh√©rent avec ton usage de Path plut√¥t que os.path\n",
    "customers_path = in_dir / \"customers.csv\"\n",
    "\n",
    "if not customers_path.exists():\n",
    "    display(Markdown(f\"Fichier manquant : `{customers_path}`.\"))\n",
    "else:\n",
    "    # üî• Lecture avec Spark (remplace pd.read_csv)\n",
    "    customers_sdf = read_csv_spark(spark, customers_path, sep=sep, enc=enc)\n",
    "\n",
    "    # üî• Equivalent de customers.head(30)\n",
    "    # Spark ne retourne pas directement un DataFrame \"affichable\" dans Jupyter,\n",
    "    # donc on prend un √©chantillon limit√© et on le convertit en pandas\n",
    "    customers_head_pdf = customers_sdf.show(50, truncate=False)\n",
    "    display(customers_head_pdf)\n",
    "\n",
    "    # üî• Equivalent de customers.shape\n",
    "    # - .count() = nombre de lignes\n",
    "    # - len(df.columns) = nombre de colonnes\n",
    "    n_rows = customers_sdf.count()          # ‚ö†Ô∏è action ‚Üí d√©clenche un job Spark\n",
    "    n_cols = len(customers_sdf.columns)\n",
    "\n",
    "    display(Markdown(f\"Taille: ({n_rows}, {n_cols})\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57732242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+------+----------+-------------------+\n",
      "|refund_id|order_id     |amount|reason    |created_at         |\n",
      "+---------+-------------+------+----------+-------------------+\n",
      "|R000001  |O202503010089|error |delay     |2025-03-01 14:03:41|\n",
      "|R000002  |O202503010038|-8.89 |gesture   |2025-03-01 22:16:56|\n",
      "|R000003  |O202503010008|again |item_issue|2025-03-01 20:06:25|\n",
      "|R000004  |O202503010073|-2.47 |coupon    |2025-03-01 20:02:46|\n",
      "|R000005  |O202503010005|-3.83 |gesture   |2025-03-01 09:58:15|\n",
      "|R000006  |O202503010099|-6.53 |item_issue|2025-03-01 20:32:00|\n",
      "|R000007  |O202503010003|-3.13 |item_issue|2025-03-01 08:49:52|\n",
      "|R000008  |O202503010080|-16.96|gesture   |2025-03-01 11:00:56|\n",
      "|R000009  |O202503010021|-5.93 |item_issue|2025-03-01 12:13:42|\n",
      "|R000010  |O202503010096|-8.82 |item_issue|2025-03-01 18:18:47|\n",
      "|R000011  |O202503010005|-8.92 |gesture   |2025-03-01 14:26:54|\n",
      "|R000012  |O202503010091|-5.72 |coupon    |2025-03-01 12:23:01|\n",
      "|R000013  |O202503010005|-5.1  |item_issue|2025-03-01 16:32:52|\n",
      "|R000014  |O202503010093|-14.86|item_issue|2025-03-01 14:30:53|\n",
      "|R000015  |O202503010021|-11.72|gesture   |2025-03-01 08:58:41|\n",
      "|R000016  |O202503010039|-16.65|coupon    |2025-03-01 22:50:16|\n",
      "|R000017  |O202503010073|-2.55 |gesture   |2025-03-01 16:07:05|\n",
      "|R000018  |O202503010025|-2.6  |item_issue|2025-03-01 08:36:05|\n",
      "|R000019  |O202503010006|-6.95 |item_issue|2025-03-01 10:52:07|\n",
      "|R000020  |O202503010021|-7.9  |delay     |2025-03-01 20:19:39|\n",
      "|R000021  |O202503010018|-3.65 |coupon    |2025-03-01 15:55:23|\n",
      "|R000022  |O202503010008|-3.69 |delay     |2025-03-01 15:52:55|\n",
      "|R000023  |O202503010053|-2.81 |gesture   |2025-03-01 21:52:06|\n",
      "|R000024  |O202503010097|-18.42|coupon    |2025-03-01 09:52:11|\n",
      "|R000025  |O202503010071|-6.77 |delay     |2025-03-01 09:51:06|\n",
      "|R000026  |O202503010074|-1.04 |coupon    |2025-03-01 08:38:04|\n",
      "|R000027  |O202503010061|-13.85|delay     |2025-03-01 21:38:55|\n",
      "|R000028  |O202503010042|-7.74 |gesture   |2025-03-01 22:04:42|\n",
      "|R000029  |O202503010075|-17.9 |delay     |2025-03-01 08:48:47|\n",
      "|R000030  |O202503010066|-13.84|coupon    |2025-03-01 16:11:24|\n",
      "|R000031  |O202503010026|-3.22 |delay     |2025-03-01 22:36:02|\n",
      "|R000032  |O202503010078|-10.86|coupon    |2025-03-01 16:30:14|\n",
      "|R000033  |O202503010059|-7.25 |coupon    |2025-03-01 14:51:20|\n",
      "|R000034  |O202503010094|-15.58|item_issue|2025-03-01 11:40:45|\n",
      "|R000035  |O202503010056|-6.05 |delay     |2025-03-01 19:34:17|\n",
      "|R000036  |O202503010024|-12.09|coupon    |2025-03-01 18:11:17|\n",
      "|R000037  |O202503010059|-14.81|coupon    |2025-03-01 14:11:30|\n",
      "|R000038  |O202503010070|-5.41 |item_issue|2025-03-01 13:54:56|\n",
      "|R000039  |O202503010044|-5.15 |coupon    |2025-03-01 17:19:41|\n",
      "|R000040  |O202503010043|-12.68|item_issue|2025-03-01 23:37:55|\n",
      "|R000041  |O202503010050|-2.81 |delay     |2025-03-01 22:52:50|\n",
      "|R000042  |O202503010015|-6.76 |item_issue|2025-03-01 14:12:11|\n",
      "|R000043  |O202503010005|-17.57|gesture   |2025-03-01 13:21:11|\n",
      "|R000044  |O202503020063|-17.2 |delay     |2025-03-02 21:01:05|\n",
      "|R000045  |O202503020016|-17.81|coupon    |2025-03-02 16:08:08|\n",
      "|R000046  |O202503020076|-11.08|gesture   |2025-03-02 08:36:05|\n",
      "|R000047  |O202503020087|-9.41 |item_issue|2025-03-02 11:41:51|\n",
      "|R000048  |O202503020014|-12.46|gesture   |2025-03-02 18:13:18|\n",
      "|R000049  |O202503020069|-15.64|gesture   |2025-03-02 18:13:17|\n",
      "|R000050  |O202503020100|-7.07 |delay     |2025-03-02 13:45:16|\n",
      "+---------+-------------+------+----------+-------------------+\n",
      "only showing top 50 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Taille: (1122, 5)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ici on utilise Path comme dans tout ton notebook\n",
    "refunds_path = in_dir / \"refunds.csv\"\n",
    "\n",
    "if not refunds_path.exists():\n",
    "    display(Markdown(f\"Fichier manquant : `{refunds_path}`.\"))\n",
    "else:\n",
    "    # üî• Lecture Spark (remplace pd.read_csv)\n",
    "    refunds_sdf = read_csv_spark(spark, refunds_path, sep=sep, enc=enc)\n",
    "\n",
    "    # üî• Equivalent de refunds.head()\n",
    "    refunds_head_pdf = refunds_sdf.show(50, truncate=False)\n",
    "    display(refunds_head_pdf)\n",
    "\n",
    "    # üî• Equivalent de refunds.shape\n",
    "    n_rows = refunds_sdf.count()         # ‚ö†Ô∏è action ‚Üí spark job\n",
    "    n_cols = len(refunds_sdf.columns)\n",
    "\n",
    "    display(Markdown(f\"Taille: ({n_rows}, {n_cols})\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43dced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+---------------------------------------------------------------------------+-------------+--------------+\n",
      "|channel|created_at         |customer_id|items                                                                      |order_id     |payment_status|\n",
      "+-------+-------------------+-----------+---------------------------------------------------------------------------+-------------+--------------+\n",
      "|app    |2025-03-01 20:36:44|C0793      |[{4, SKU001, 24.9}]                                                        |O202503010001|pending       |\n",
      "|web    |2025-03-01 11:30:49|C0676      |[{4, SKU042, -7.5}, {4, SKU042, -7.5}, {5, SKU005, 12.5}]                  |O202503010001|paid          |\n",
      "|web    |2025-03-01 07:27:00|C0642      |[{1, SKU014, 5.0}]                                                         |O202503010003|paid          |\n",
      "|web    |2025-03-01 14:28:46|C0283      |[{2, SKU024, 4.0}]                                                         |O202503010004|pending       |\n",
      "|web    |2025-03-01 22:29:42|C0571      |[{1, SKU001, 2.5}]                                                         |O202503010005|paid          |\n",
      "|web    |2025-03-01 09:24:19|C0704      |[{1, SKU039, 9.9}, {4, SKU037, 15.0}, {1, SKU010, 9.9}, {4, SKU011, 15.0}] |O202503010006|paid          |\n",
      "|app    |2025-03-01 15:50:48|C0464      |[{1, SKU018, 24.9}, {5, SKU046, 9.9}, {1, SKU047, 5.0}]                    |O202503010007|paid          |\n",
      "|app    |2025-03-01 20:56:15|C0317      |[{2, SKU039, 15.0}, {3, SKU001, 8.0}, {5, SKU011, 2.5}, {5, SKU010, 5.0}]  |O202503010008|paid          |\n",
      "|app    |2025-03-01 22:22:34|C0561      |[{2, SKU013, 8.0}, {4, SKU044, 15.0}, {5, SKU017, 2.5}, {5, SKU030, 4.0}]  |O202503010009|paid          |\n",
      "|app    |2025-03-01 09:14:25|C0364      |[{5, SKU031, 24.9}, {2, SKU014, 4.0}, {3, SKU045, 9.9}]                    |O202503010010|paid          |\n",
      "|web    |2025-03-01 19:49:33|C0368      |[{3, SKU032, 4.0}]                                                         |O202503010011|paid          |\n",
      "|web    |2025-03-01 09:09:20|C0475      |[{5, SKU018, 24.9}, {2, SKU003, 24.9}, {2, SKU037, 8.0}]                   |O202503010012|paid          |\n",
      "|web    |2025-03-01 20:06:28|C0287      |[{3, SKU003, 7.5}, {1, SKU025, 2.5}, {3, SKU036, 15.0}]                    |O202503010013|failed        |\n",
      "|web    |2025-03-01 19:59:07|C0066      |[{5, SKU021, 7.5}, {5, SKU043, 12.5}, {3, SKU011, 7.5}]                    |O202503010014|pending       |\n",
      "|web    |2025-03-01 12:02:16|C0228      |[{3, SKU004, 19.9}]                                                        |O202503010015|paid          |\n",
      "|app    |2025-03-01 22:03:06|C0663      |[{3, SKU013, 24.9}]                                                        |O202503010016|paid          |\n",
      "|app    |2025-03-01 10:30:20|C0777      |[{3, SKU008, 15.0}, {2, SKU029, 5.0}, {4, SKU001, 7.5}, {2, SKU011, 4.0}]  |O202503010017|paid          |\n",
      "|app    |2025-03-01 11:49:28|C0383      |[{1, SKU041, 15.0}]                                                        |O202503010018|paid          |\n",
      "|app    |2025-03-01 14:30:07|C0331      |[{2, SKU022, 2.5}, {5, SKU029, 5.0}]                                       |O202503010019|paid          |\n",
      "|web    |2025-03-01 15:26:26|C0153      |[{5, SKU018, 8.0}]                                                         |O202503010020|paid          |\n",
      "|app    |2025-03-01 15:31:06|C0172      |[{2, SKU008, 19.9}, {2, SKU043, 19.9}, {1, SKU019, 8.0}, {4, SKU024, 8.0}] |O202503010021|paid          |\n",
      "|web    |2025-03-01 10:24:18|C0244      |[{3, SKU047, 5.0}]                                                         |O202503010022|paid          |\n",
      "|web    |2025-03-01 21:51:32|C0017      |[{5, SKU001, 8.0}, {1, SKU028, 12.5}, {2, SKU037, 5.0}, {2, SKU034, 5.0}]  |O202503010023|paid          |\n",
      "|app    |2025-03-01 09:56:38|C0082      |[{2, SKU012, 5.0}, {2, SKU046, 24.9}, {1, SKU001, 19.9}]                   |O202503010024|pending       |\n",
      "|app    |2025-03-01 08:33:51|C0739      |[{4, SKU041, 4.0}, {4, SKU049, 5.0}, {2, SKU018, 5.0}]                     |O202503010025|paid          |\n",
      "|web    |2025-03-01 08:10:44|C0376      |[{4, SKU034, 19.9}, {2, SKU023, 9.9}, {5, SKU025, 2.5}]                    |O202503010026|paid          |\n",
      "|web    |2025-03-01 22:28:32|C0111      |[{1, SKU016, 7.5}]                                                         |O202503010027|paid          |\n",
      "|web    |2025-03-01 10:19:16|C0172      |[{2, SKU007, 8.0}]                                                         |O202503010028|paid          |\n",
      "|app    |2025-03-01 21:33:15|C0614      |[{1, SKU023, 5.0}]                                                         |O202503010029|paid          |\n",
      "|app    |2025-03-01 21:31:37|C0127      |[{1, SKU008, 12.5}]                                                        |O202503010030|paid          |\n",
      "|app    |2025-03-01 14:55:14|C0555      |[{1, SKU011, 12.5}, {5, SKU039, 19.9}, {3, SKU004, 9.9}, {4, SKU022, 24.9}]|O202503010031|paid          |\n",
      "|app    |2025-03-01 19:54:35|C0329      |[{3, SKU044, 7.5}, {1, SKU043, 9.9}]                                       |O202503010032|paid          |\n",
      "|web    |2025-03-01 09:20:27|C0192      |[{4, SKU009, 12.5}, {1, SKU030, 2.5}]                                      |O202503010033|paid          |\n",
      "|app    |2025-03-01 15:58:43|C0657      |[{1, SKU040, 8.0}]                                                         |O202503010034|paid          |\n",
      "|web    |2025-03-01 20:15:02|C0014      |[{2, SKU023, 4.0}, {3, SKU033, 4.0}, {2, SKU035, 15.0}]                    |O202503010035|paid          |\n",
      "|app    |2025-03-01 16:58:26|C0135      |[{1, SKU016, 19.9}, {5, SKU030, 24.9}, {2, SKU025, 19.9}]                  |O202503010036|paid          |\n",
      "|app    |2025-03-01 16:39:30|C0472      |[{3, SKU016, 7.5}]                                                         |O202503010037|paid          |\n",
      "|app    |2025-03-01 19:37:25|C0560      |[{3, SKU016, 19.9}, {3, SKU018, 7.5}]                                      |O202503010038|paid          |\n",
      "|app    |2025-03-01 07:10:35|C0791      |[{1, SKU043, 19.9}, {3, SKU029, 4.0}, {2, SKU044, 12.5}, {2, SKU023, 7.5}] |O202503010039|paid          |\n",
      "|app    |2025-03-01 10:47:54|C0284      |[{2, SKU041, 12.5}, {4, SKU001, 19.9}, {4, SKU032, 24.9}]                  |O202503010040|pending       |\n",
      "|app    |2025-03-01 10:24:54|C0287      |[{3, SKU047, 8.0}, {5, SKU034, 24.9}, {1, SKU021, 15.0}]                   |O202503010041|paid          |\n",
      "|app    |2025-03-01 12:34:19|C0308      |[{2, SKU038, 4.0}, {3, SKU022, 24.9}, {2, SKU021, 12.5}, {1, SKU001, 2.5}] |O202503010042|paid          |\n",
      "|app    |2025-03-01 16:58:34|C0510      |[{5, SKU034, 12.5}, {1, SKU023, 24.9}, {1, SKU029, 4.0}, {4, SKU007, 9.9}] |O202503010043|failed        |\n",
      "|app    |2025-03-01 11:56:12|C0665      |[{5, SKU029, 24.9}, {1, SKU034, 5.0}, {1, SKU024, 8.0}, {3, SKU008, 9.9}]  |O202503010044|paid          |\n",
      "|app    |2025-03-01 20:40:10|C0522      |[{2, SKU033, 12.5}, {5, SKU041, 24.9}]                                     |O202503010045|paid          |\n",
      "|web    |2025-03-01 08:44:26|C0584      |[{5, SKU046, 2.5}, {1, SKU026, 24.9}, {2, SKU002, 5.0}]                    |O202503010046|paid          |\n",
      "|web    |2025-03-01 15:55:41|C0567      |[{5, SKU027, 4.0}, {5, SKU034, 4.0}]                                       |O202503010047|paid          |\n",
      "|app    |2025-03-01 12:33:31|C0078      |[{1, SKU042, 24.9}]                                                        |O202503010048|paid          |\n",
      "|web    |2025-03-01 14:22:17|C0733      |[{1, SKU041, 24.9}, {4, SKU013, 24.9}, {2, SKU004, 12.5}]                  |O202503010049|paid          |\n",
      "|web    |2025-03-01 21:03:39|C0045      |[{2, SKU003, 24.9}, {1, SKU021, 15.0}]                                     |O202503010050|paid          |\n",
      "+-------+-------------------+-----------+---------------------------------------------------------------------------+-------------+--------------+\n",
      "only showing top 50 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Taille: (103, 6)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Sch√©ma d√©tect√© par Spark :\n",
      "root\n",
      " |-- channel: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      " |    |    |-- sku: string (nullable = true)\n",
      " |    |    |-- unit_price: double (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Toujours Path pour coh√©rence\n",
    "order_path = in_dir / \"orders_2025-03-01.json\"\n",
    "\n",
    "if not order_path.exists():\n",
    "    display(Markdown(f\"Fichier manquant : `{order_path}`.\"))\n",
    "else:\n",
    "    # üî• Lecture JSON avec Spark (remplace pd.read_json)\n",
    "    order_sdf = (\n",
    "        spark.read\n",
    "             .option(\"multiline\", True)      # si JSON complexe multi-lignes\n",
    "             .json(str(order_path))\n",
    "    )\n",
    "\n",
    "    # üî• Equivalent de df.head()\n",
    "    order_head_pdf = order_sdf.show(50, truncate=False)\n",
    "    display(order_head_pdf)\n",
    "\n",
    "    # üî• Equivalent de df.shape\n",
    "    n_rows = order_sdf.count()                 # ‚ö†Ô∏è job Spark\n",
    "    n_cols = len(order_sdf.columns)\n",
    "\n",
    "    display(Markdown(f\"Taille: ({n_rows}, {n_cols})\"))\n",
    "\n",
    "    # (Tr√®s utile !) voir le sch√©ma :\n",
    "    print(\"üìå Sch√©ma d√©tect√© par Spark :\")\n",
    "    order_sdf.printSchema()\n",
    "    #Le sch√©ma en PySpark, c‚Äôest la ‚Äúcarte d‚Äôidentit√©‚Äù des donn√©es : \n",
    "    #il d√©crit le type et la nullabilit√© de chaque colonne, \n",
    "    # ce qui permet √† Spark d‚Äôoptimiser massivement l‚Äôex√©cution, \n",
    "    # de d√©tecter les incoh√©rences et de manipuler des donn√©es complexes \n",
    "    # de fa√ßon s√ªre et ultra-performante ‚Äî en bref, c‚Äôest g√©nial \n",
    "    # parce que Spark sait exactement quoi tu traites et comment le traiter, \n",
    "    # sans jamais tout charger en m√©moire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a42cccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 13:39:13 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: /mnt/c/Users/alexa/Simplon/Esther/Exos/Starter stack pour Data Engineers - Partie 1/data/march-input/orders_2025-03-*.json.\n",
      "java.io.FileNotFoundException: File /mnt/c/Users/alexa/Simplon/Esther/Exos/Starter stack pour Data Engineers - Partie 1/data/march-input/orders_2025-03-*.json does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+-------------------------------------------------------------------------+-------------+--------------+\n",
      "|channel|created_at         |customer_id|items                                                                    |order_id     |payment_status|\n",
      "+-------+-------------------+-----------+-------------------------------------------------------------------------+-------------+--------------+\n",
      "|web    |2025-03-07 22:25:41|C0636      |[{1, SKU022, 5.0}, {2, SKU023, 4.0}, {3, SKU025, 15.0}, {5, SKU006, 7.5}]|O202503070001|paid          |\n",
      "|app    |2025-03-07 19:17:49|C0499      |[{5, SKU017, 4.0}]                                                       |O202503070002|paid          |\n",
      "|app    |2025-03-07 19:39:25|C0417      |[{5, SKU007, 4.0}, {3, SKU022, 7.5}, {4, SKU005, 4.0}, {2, SKU001, 12.5}]|O202503070003|pending       |\n",
      "|web    |2025-03-07 11:00:36|C0056      |[{4, SKU030, 5.0}, {2, SKU046, 8.0}, {5, SKU029, 7.5}]                   |O202503070004|paid          |\n",
      "|web    |2025-03-07 12:03:11|C0270      |[{4, SKU025, 19.9}, {2, SKU008, 5.0}]                                    |O202503070005|paid          |\n",
      "+-------+-------------------+-----------+-------------------------------------------------------------------------+-------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Taille: (3193, 6)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pattern pour lire *tous* les orders du mois\n",
    "# ex : orders_2025-03-01.json, orders_2025-03-02.json, ..., orders_2025-03-31.json\n",
    "pattern = str(in_dir / \"orders_2025-03-*.json\")\n",
    "\n",
    "# üî• Lecture de tous les JSON d'un coup\n",
    "orders_sdf = spark.read.option(\"multiline\", True).json(pattern)\n",
    "\n",
    "# Aper√ßu (√©quivalent head)\n",
    "orders_sdf.show(5, truncate=False)\n",
    "\n",
    "# Taille\n",
    "n_rows = orders_sdf.count()\n",
    "n_cols = len(orders_sdf.columns)\n",
    "\n",
    "display(Markdown(f\"Taille: ({n_rows}, {n_cols})\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "436dc6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Affichage clients (apr√®s nettoyage)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----------------+--------+---------+\n",
      "|customer_id|first_name|last_name|email            |city    |is_active|\n",
      "+-----------+----------+---------+-----------------+--------+---------+\n",
      "|C0001      |User1     |Test1    |user1@example.com|Nantes  |true     |\n",
      "|C0002      |User2     |Test2    |user2@example.com|Toulouse|true     |\n",
      "|C0003      |User3     |Test3    |user3@example.com|Bordeaux|true     |\n",
      "|C0004      |User4     |Test4    |user4@example.com|Bordeaux|true     |\n",
      "|C0005      |User5     |Test5    |user5@example.com|Lyon    |true     |\n",
      "+-----------+----------+---------+-----------------+--------+---------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Taille: (800, 6)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# On part de customers_sdf (DataFrame Spark charg√© avant)\n",
    "customers_clean = (\n",
    "    customers_sdf\n",
    "    # 1Ô∏è‚É£ Normalisation de \"is_active\" (√©quivalent de apply(controle_bool))\n",
    "    .withColumn(\n",
    "        \"is_active\",\n",
    "        F.when(F.col(\"is_active\").cast(\"boolean\").isNotNull(), F.col(\"is_active\").cast(\"boolean\"))  # cas bool direct\n",
    "         .when(F.col(\"is_active\").isin(1, \"1\"), True)                                              # int/str -> True\n",
    "         .when(F.lower(F.col(\"is_active\")).isin(\"true\", \"yes\", \"y\", \"t\"), True)                   # strings positives\n",
    "         .when(F.col(\"is_active\").isin(0, \"0\"), False)                                             # False explicite\n",
    "         .when(F.lower(F.col(\"is_active\")).isin(\"false\", \"no\", \"n\", \"f\"), False)                  # strings n√©gatives\n",
    "         .otherwise(False)                                                                         # fallback = False (comme ton code)\n",
    "    )\n",
    "\n",
    "    # 2Ô∏è‚É£ Force le type de customer_id et city\n",
    "    # Pandas : customers.astype({\"customer_id\":\"string\",\"city\":\"string\"})\n",
    "    # PySpark : cast sur chaque colonne\n",
    "    .withColumn(\"customer_id\", F.col(\"customer_id\").cast(\"string\"))\n",
    "    .withColumn(\"city\", F.col(\"city\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Affichage (√©quivalent display + head)\n",
    "display(Markdown(\"Affichage clients (apr√®s nettoyage)\"))\n",
    "customers_clean.show(5, truncate=False)\n",
    "\n",
    "# 4Ô∏è‚É£ Taille √©quivalente\n",
    "n_rows = customers_clean.count()\n",
    "n_cols = len(customers_clean.columns)\n",
    "display(Markdown(f\"Taille: ({n_rows}, {n_cols})\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "078704fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Aper√ßu remboursements (apr√®s coercition num√©rique)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+------+----------+-------------------+\n",
      "|refund_id|order_id     |amount|reason    |created_at         |\n",
      "+---------+-------------+------+----------+-------------------+\n",
      "|R000001  |O202503010089|0.0   |delay     |2025-03-01 14:03:41|\n",
      "|R000002  |O202503010038|-8.89 |gesture   |2025-03-01 22:16:56|\n",
      "|R000003  |O202503010008|0.0   |item_issue|2025-03-01 20:06:25|\n",
      "|R000004  |O202503010073|-2.47 |coupon    |2025-03-01 20:02:46|\n",
      "|R000005  |O202503010005|-3.83 |gesture   |2025-03-01 09:58:15|\n",
      "+---------+-------------+------+----------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Taille: (1122, 5)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "refunds_clean = (\n",
    "    refunds_sdf\n",
    "    # 1Ô∏è‚É£ Utiliser try_cast ‚Üí NULL si ce n'est pas convertible, oblig√© \n",
    "    #d'utiliser une fonction try_cast qui vient du SQL non pas du python pour\n",
    "    #que pyspark continue sans soulever d'erreur\n",
    "    .withColumn(\n",
    "        \"amount\",\n",
    "        F.expr(\"try_cast(amount as double)\")\n",
    "    )\n",
    "    # 2Ô∏è‚É£ Remplacer les NULL par 0.0 (comme fillna apr√®s to_numeric)\n",
    "    .fillna({\"amount\": 0.0})\n",
    "    # 3Ô∏è‚É£ Garder created_at en string\n",
    "    .withColumn(\"created_at\", F.col(\"created_at\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "display(Markdown(\"Aper√ßu remboursements (apr√®s coercition num√©rique)\"))\n",
    "refunds_clean.show(5, truncate=False)\n",
    "n_rows = refunds_clean.count()\n",
    "n_cols = len(refunds_clean.columns)\n",
    "display(Markdown(f\"Taille: ({n_rows}, {n_cols})\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7921c1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Filtrage pay√©es : 3193 ‚Üí 2900"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+-------------------------------------------------------------------------+-------------+--------------+\n",
      "|channel|created_at         |customer_id|items                                                                    |order_id     |payment_status|\n",
      "+-------+-------------------+-----------+-------------------------------------------------------------------------+-------------+--------------+\n",
      "|web    |2025-03-07 22:25:41|C0636      |[{1, SKU022, 5.0}, {2, SKU023, 4.0}, {3, SKU025, 15.0}, {5, SKU006, 7.5}]|O202503070001|paid          |\n",
      "|app    |2025-03-07 19:17:49|C0499      |[{5, SKU017, 4.0}]                                                       |O202503070002|paid          |\n",
      "|web    |2025-03-07 11:00:36|C0056      |[{4, SKU030, 5.0}, {2, SKU046, 8.0}, {5, SKU029, 7.5}]                   |O202503070004|paid          |\n",
      "|web    |2025-03-07 12:03:11|C0270      |[{4, SKU025, 19.9}, {2, SKU008, 5.0}]                                    |O202503070005|paid          |\n",
      "|web    |2025-03-07 10:51:35|C0240      |[{2, SKU041, 9.9}, {2, SKU021, 4.0}, {3, SKU039, 12.5}, {5, SKU037, 7.5}]|O202503070006|paid          |\n",
      "+-------+-------------------+-----------+-------------------------------------------------------------------------+-------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ Nombre de lignes avant filtrage\n",
    "ln_initial = orders_sdf.count()   # ‚ö†Ô∏è Spark action (job)\n",
    "\n",
    "# 2Ô∏è‚É£ Filtrage (√©quivalent Pandas : orders[orders[\"payment_status\"]==\"paid\"])\n",
    "orders_filtered = orders_sdf.filter(F.col(\"payment_status\") == \"paid\")\n",
    "\n",
    "# 3Ô∏è‚É£ Nombre de lignes apr√®s filtrage\n",
    "ln_final = orders_filtered.count()  # ‚ö†Ô∏è autre job Spark\n",
    "\n",
    "# 4Ô∏è‚É£ Affichage\n",
    "display(Markdown(f\"Filtrage pay√©es : {ln_initial} ‚Üí {ln_final}\"))\n",
    "\n",
    "# Equivalent de .head()\n",
    "orders_filtered.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fefd0be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Avant explosion des items"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+-------------------------------------------------------------------------+-------------+--------------+\n",
      "|channel|created_at         |customer_id|items                                                                    |order_id     |payment_status|\n",
      "+-------+-------------------+-----------+-------------------------------------------------------------------------+-------------+--------------+\n",
      "|web    |2025-03-07 22:25:41|C0636      |[{1, SKU022, 5.0}, {2, SKU023, 4.0}, {3, SKU025, 15.0}, {5, SKU006, 7.5}]|O202503070001|paid          |\n",
      "|app    |2025-03-07 19:17:49|C0499      |[{5, SKU017, 4.0}]                                                       |O202503070002|paid          |\n",
      "|web    |2025-03-07 11:00:36|C0056      |[{4, SKU030, 5.0}, {2, SKU046, 8.0}, {5, SKU029, 7.5}]                   |O202503070004|paid          |\n",
      "|web    |2025-03-07 12:03:11|C0270      |[{4, SKU025, 19.9}, {2, SKU008, 5.0}]                                    |O202503070005|paid          |\n",
      "|web    |2025-03-07 10:51:35|C0240      |[{2, SKU041, 9.9}, {2, SKU021, 4.0}, {3, SKU039, 12.5}, {5, SKU037, 7.5}]|O202503070006|paid          |\n",
      "+-------+-------------------+-----------+-------------------------------------------------------------------------+-------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Apr√®s explosion des items"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+-------------+--------------+--------+--------+---------------+\n",
      "|channel|created_at         |customer_id|order_id     |payment_status|item_sku|item_qty|item_unit_price|\n",
      "+-------+-------------------+-----------+-------------+--------------+--------+--------+---------------+\n",
      "|web    |2025-03-07 22:25:41|C0636      |O202503070001|paid          |SKU022  |1       |5.0            |\n",
      "|web    |2025-03-07 22:25:41|C0636      |O202503070001|paid          |SKU023  |2       |4.0            |\n",
      "|web    |2025-03-07 22:25:41|C0636      |O202503070001|paid          |SKU025  |3       |15.0           |\n",
      "|web    |2025-03-07 22:25:41|C0636      |O202503070001|paid          |SKU006  |5       |7.5            |\n",
      "|app    |2025-03-07 19:17:49|C0499      |O202503070002|paid          |SKU017  |5       |4.0            |\n",
      "+-------+-------------------+-----------+-------------+--------------+--------+--------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Colonnes: ['channel', 'created_at', 'customer_id', 'order_id', 'payment_status', 'item_sku', 'item_qty', 'item_unit_price'] ..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On part du DataFrame Spark des commandes pay√©es\n",
    "orders_paid = orders_sdf.filter(F.col(\"payment_status\") == \"paid\")\n",
    "\n",
    "display(Markdown(\"Avant explosion des items\"))\n",
    "orders_paid.show(5, truncate=False)\n",
    "\n",
    "# 1Ô∏è‚É£ Explosion de la colonne array<struct> \"items\"\n",
    "#    √âquivalent de : orders2 = orders2.explode(\"items\")\n",
    "orders_exploded = orders_paid.withColumn(\"item\", F.explode(\"items\"))\n",
    "\n",
    "# 2Ô∏è‚É£ Flatten de la struct \"item\" en colonnes simples (item_sku, item_qty, item_unit_price)\n",
    "#    En pandas : json_normalize(orders2[\"items\"]).add_prefix(\"item_\")\n",
    "#    En Spark : on acc√®de directement aux champs de la struct\n",
    "base_cols = [c for c in orders_paid.columns if c != \"items\"]\n",
    "\n",
    "orders_flat = (\n",
    "    orders_exploded\n",
    "    .select(\n",
    "        *[F.col(c) for c in base_cols],               # toutes les colonnes d‚Äôorigine sauf \"items\"\n",
    "        F.col(\"item.sku\").alias(\"item_sku\"),          # champs de la struct\n",
    "        F.col(\"item.qty\").alias(\"item_qty\"),\n",
    "        F.col(\"item.unit_price\").alias(\"item_unit_price\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "display(Markdown(\"Apr√®s explosion des items\"))\n",
    "orders_flat.show(5, truncate=False)\n",
    "\n",
    "display(Markdown(f\"Colonnes: {orders_flat.columns[:12]} ...\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e66b6",
   "metadata": {},
   "source": [
    "Avec Pandas, tu manipules des objets Python (list + dict) ‚Üí tu dois normaliser √† la main (explode, json_normalize, concat).\n",
    "\n",
    "Avec PySpark, tu manipules des types fortement typ√©s (array<struct<...>>) ‚Üí tu peux simplement :\n",
    "\n",
    "explode l‚Äôarray\n",
    "\n",
    "acc√©der aux champs de la struct (item.sku, etc.)\n",
    "\n",
    "select les colonnes que tu veux garder\n",
    "\n",
    "C‚Äôest plus court, plus lisible, et scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "800aed41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Lignes prix n√©gatifs : 69"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Rejets sauvegard√©s : `/mnt/c/Users/alexa/Simplon/Esther/Exos/Starter stack pour Data Engineers - Partie 1/data/out/rejects_items.csv`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+-------------+--------------+--------+--------+---------------+\n",
      "|channel|created_at         |customer_id|order_id     |payment_status|item_sku|item_qty|item_unit_price|\n",
      "+-------+-------------------+-----------+-------------+--------------+--------+--------+---------------+\n",
      "|web    |2025-03-07 22:25:41|C0636      |O202503070001|paid          |SKU022  |1       |5.0            |\n",
      "|web    |2025-03-07 22:25:41|C0636      |O202503070001|paid          |SKU023  |2       |4.0            |\n",
      "|web    |2025-03-07 22:25:41|C0636      |O202503070001|paid          |SKU025  |3       |15.0           |\n",
      "|web    |2025-03-07 22:25:41|C0636      |O202503070001|paid          |SKU006  |5       |7.5            |\n",
      "|app    |2025-03-07 19:17:49|C0499      |O202503070002|paid          |SKU017  |5       |4.0            |\n",
      "+-------+-------------------+-----------+-------------+--------------+--------+--------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ D√©tection des lignes n√©gatives\n",
    "neg_sdf = orders_flat.filter(F.col(\"item_unit_price\") < 0)\n",
    "\n",
    "# 2Ô∏è‚É£ Compter (√©quivalent neg_mask.sum())\n",
    "n_neg = neg_sdf.count()\n",
    "\n",
    "display(Markdown(f\"Lignes prix n√©gatifs : {n_neg}\"))\n",
    "\n",
    "# 3Ô∏è‚É£ Si rejets ‚Üí √©crire dans un CSV, sinon continuer normalement\n",
    "if n_neg > 0:\n",
    "    rejects_path = str(out_dir / \"rejects_items.csv\")\n",
    "\n",
    "    # Sauvegarde en CSV (Spark √©crit un dossier ‚Üí on force en fichier unique)\n",
    "    (neg_sdf\n",
    "        .coalesce(1)                  # un seul fichier\n",
    "        .write\n",
    "        .option(\"header\", True)\n",
    "        .mode(\"overwrite\")\n",
    "        .csv(rejects_path)\n",
    "    )\n",
    "\n",
    "    display(Markdown(f\"Rejets sauvegard√©s : `{rejects_path}`\"))\n",
    "\n",
    "# 4Ô∏è‚É£ Garder uniquement les lignes positives\n",
    "orders_clean = orders_flat.filter(F.col(\"item_unit_price\") >= 0)\n",
    "\n",
    "# 5Ô∏è‚É£ Aper√ßu (√©quivalent orders2.head())\n",
    "orders_clean.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "170e4593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "D√©duplication : **7196 ‚Üí 2811**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+-------------+--------------+--------+--------+---------------+\n",
      "|channel|created_at         |customer_id|order_id     |payment_status|item_sku|item_qty|item_unit_price|\n",
      "+-------+-------------------+-----------+-------------+--------------+--------+--------+---------------+\n",
      "|web    |2025-03-01 11:30:49|C0676      |O202503010001|paid          |SKU005  |5       |12.5           |\n",
      "|web    |2025-03-01 07:27:00|C0642      |O202503010003|paid          |SKU014  |1       |5.0            |\n",
      "|web    |2025-03-01 22:29:42|C0571      |O202503010005|paid          |SKU001  |1       |2.5            |\n",
      "|web    |2025-03-01 09:24:19|C0704      |O202503010006|paid          |SKU039  |1       |9.9            |\n",
      "|app    |2025-03-01 15:50:48|C0464      |O202503010007|paid          |SKU018  |1       |24.9           |\n",
      "+-------+-------------------+-----------+-------------+--------------+--------+--------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# üêº Pandas (id√©e g√©n√©rale) :\n",
    "# before = len(orders2)\n",
    "#\n",
    "# orders3 = (\n",
    "#     orders2\n",
    "#     .sort_values([\"order_id\",\"created_at\"])\n",
    "#     .drop_duplicates(subset=[\"order_id\"], keep=\"first\")\n",
    "# )\n",
    "#\n",
    "# after = len(orders3)\n",
    "#\n",
    "# Ici :\n",
    "#   ‚ñ∂ Pandas peut garder la \"premi√®re occurrence\" car il a un ordre de lignes\n",
    "#   ‚ñ∂ Il peut trier, puis supprimer les doublons selon cet ordre\n",
    "# ==========================================================\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# üî• PySpark ‚Äî √âtape 1 : compter les lignes avant d√©duplication\n",
    "#    (√©quivalent de len(orders2) en pandas)\n",
    "# ==========================================================\n",
    "before = orders_clean.count()    # ‚ö†Ô∏è Action Spark : d√©clenche un job\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# üî• PySpark ‚Äî √âtape 2 : d√©finir une fen√™tre\n",
    "#    üêº En Pandas : .sort_values(\"order_id\",\"created_at\")\n",
    "#\n",
    "# ‚ö†Ô∏è Spark ne peut PAS faire \"drop_duplicates(... keep='first')\"\n",
    "#     sans qu‚Äôon lui dise ce que signifie \"first\".\n",
    "#\n",
    "# On doit donc d√©finir :\n",
    "#   - comment grouper : partitionBy(\"order_id\")\n",
    "#   - comment ordonner : orderBy(\"created_at\")\n",
    "#\n",
    "# Spark fera ensuite un ranking (row_number) dans chaque groupe.\n",
    "# ==========================================================\n",
    "w = Window.partitionBy(\"order_id\").orderBy(\"created_at\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# üî• PySpark ‚Äî √âtape 3 :\n",
    "# Num√©roter les lignes dans chaque groupe (ordre croissant dates)\n",
    "#\n",
    "# üêº Pandas aurait implicitement utilis√© l'ordre du dataframe tri√©\n",
    "#     pour savoir quelle est \"la premi√®re ligne\".\n",
    "#\n",
    "# üî• Spark doit calculer explicitement un \"num√©ro de ligne\".\n",
    "# ==========================================================\n",
    "orders_ranked = orders_clean.withColumn(\n",
    "    \"rn\",\n",
    "    F.row_number().over(w)      # rn = 1 ‚Üí premi√®re ligne pour ce order_id\n",
    ")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# üî• PySpark ‚Äî √âtape 4 :\n",
    "# Garder uniquement la premi√®re ligne (rn == 1)\n",
    "#\n",
    "# üêº Pandas : .drop_duplicates(subset=[\"order_id\"], keep=\"first\")\n",
    "# üî• PySpark : √©quivalent = filtrer rn == 1, puis drop rnk\n",
    "# ==========================================================\n",
    "orders_dedup = (\n",
    "    orders_ranked\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# üî• PySpark ‚Äî √âtape 5 :\n",
    "# Compter les lignes apr√®s d√©duplication\n",
    "#\n",
    "# üêº Pandas : len(orders3)\n",
    "# ==========================================================\n",
    "after = orders_dedup.count()\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# üî• PySpark ‚Äî √âtape 6 : afficher un √©chantillon\n",
    "#\n",
    "# üêº Pandas : orders3.head()\n",
    "# üî• PySpark : limit().toPandas() OU show()\n",
    "# ==========================================================\n",
    "display(Markdown(f\"D√©duplication : **{before} ‚Üí {after}**\"))\n",
    "\n",
    "orders_dedup.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8478bb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Aper√ßu `per_order` (PySpark)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-------+-------------------+----------+-----------------+\n",
      "|order_id     |customer_id|channel|created_at         |items_sold|gross_revenue_eur|\n",
      "+-------------+-----------+-------+-------------------+----------+-----------------+\n",
      "|O202503010001|C0676      |web    |2025-03-01 11:30:49|5         |62.5             |\n",
      "|O202503010003|C0642      |web    |2025-03-01 07:27:00|1         |5.0              |\n",
      "|O202503010005|C0571      |web    |2025-03-01 22:29:42|1         |2.5              |\n",
      "|O202503010006|C0704      |web    |2025-03-01 09:24:19|1         |9.9              |\n",
      "|O202503010007|C0464      |app    |2025-03-01 15:50:48|1         |24.9             |\n",
      "+-------------+-----------+-------+-------------------+----------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Taille: (2811, 6)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On part de orders_dedup (√©quivalent de `orders3` en pandas)\n",
    "# üêº Pandas : orders3[\"line_gross\"] = orders3[\"item_qty\"] * orders3[\"item_unit_price\"]\n",
    "# üî• PySpark : on utilise withColumn (les DF Spark sont immuables)\n",
    "orders_with_gross = orders_dedup.withColumn(\n",
    "    \"line_gross\",\n",
    "    F.col(\"item_qty\") * F.col(\"item_unit_price\")\n",
    ")\n",
    "\n",
    "# üêº Pandas :\n",
    "# per_order = orders3.groupby(\n",
    "#     [\"order_id\",\"customer_id\",\"channel\",\"created_at\"], as_index=False\n",
    "# ).agg(\n",
    "#     items_sold=(\"item_qty\",\"sum\"),\n",
    "#     gross_revenue_eur=(\"line_gross\",\"sum\")\n",
    "# )\n",
    "#\n",
    "# üî• PySpark : groupBy + agg(F.sum(...).alias(...))\n",
    "per_order_sdf = (\n",
    "    orders_with_gross\n",
    "    .groupBy(\"order_id\", \"customer_id\", \"channel\", \"created_at\")\n",
    "    .agg(\n",
    "        F.sum(\"item_qty\").alias(\"items_sold\"),\n",
    "        F.sum(\"line_gross\").alias(\"gross_revenue_eur\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(Markdown(\"Aper√ßu `per_order` (PySpark)\"))\n",
    "\n",
    "# üêº Pandas : per_order.head()\n",
    "# üî• PySpark : limit().toPandas() pour avoir un mini DataFrame local affichable dans le notebook\n",
    "per_order_sdf.show(5, truncate=False)\n",
    "\n",
    "# üêº Pandas : per_order.shape\n",
    "# üî• PySpark :\n",
    "n_rows = per_order_sdf.count()            # action Spark ‚Üí d√©clenche un job\n",
    "n_cols = len(per_order_sdf.columns)       # cheap, utilise juste le sch√©ma\n",
    "display(Markdown(f\"Taille: ({n_rows}, {n_cols})\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f994499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Apr√®s jointure+filtre actifs : **2811 ‚Üí 2471**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+-------------------+----------+-----------------+---------+---------+\n",
      "|customer_id|order_id     |channel|created_at         |items_sold|gross_revenue_eur|city     |is_active|\n",
      "+-----------+-------------+-------+-------------------+----------+-----------------+---------+---------+\n",
      "|C0676      |O202503010001|web    |2025-03-01 11:30:49|5         |62.5             |Marseille|true     |\n",
      "|C0642      |O202503010003|web    |2025-03-01 07:27:00|1         |5.0              |Toulouse |true     |\n",
      "|C0571      |O202503010005|web    |2025-03-01 22:29:42|1         |2.5              |Toulouse |true     |\n",
      "|C0464      |O202503010007|app    |2025-03-01 15:50:48|1         |24.9             |Nantes   |true     |\n",
      "|C0317      |O202503010008|app    |2025-03-01 20:56:15|2         |30.0             |Marseille|true     |\n",
      "+-----------+-------------+-------+-------------------+----------+-----------------+---------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# üêº Pandas : len_init = len(per_order)\n",
    "# üî• PySpark : on utilise count(), qui d√©clenche un job Spark\n",
    "# =====================================================================\n",
    "len_init = per_order_sdf.count()   # ‚ö†Ô∏è action Spark (scan distribu√©)\n",
    "\n",
    "# =====================================================================\n",
    "# üêº Pandas :\n",
    "# per_order = per_order.merge(\n",
    "#     customers[[\"customer_id\",\"city\",\"is_active\"]],\n",
    "#     on=\"customer_id\",\n",
    "#     how=\"left\"\n",
    "# )\n",
    "# üî• PySpark :\n",
    "# - join() au lieu de merge()\n",
    "# - on travaille sur des DataFrames distribu√©s\n",
    "# - n√©cessit√© de s√©lectionner les colonnes voulues c√¥t√© customers\n",
    "# =====================================================================\n",
    "per_order_joined = (\n",
    "    per_order_sdf.alias(\"o\")\n",
    "    .join(\n",
    "        customers_clean.select(\"customer_id\", \"city\", \"is_active\").alias(\"c\"),\n",
    "        on=\"customer_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# =====================================================================\n",
    "# üêº Pandas :\n",
    "# per_order = per_order[per_order[\"is_active\"] == True].copy()\n",
    "#\n",
    "# - filtre sur les clients actifs\n",
    "# - .copy() pour √©viter SettingWithCopyWarning\n",
    "# üî• PySpark :\n",
    "# - pas de .copy() (DF immuables)\n",
    "# - on filtre avec filter()/where() sur la colonne bool√©enne \"is_active\"\n",
    "# =====================================================================\n",
    "per_order_active = per_order_joined.filter(F.col(\"is_active\") == True)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# üêº Pandas : ln_aft = len(per_order)\n",
    "# üî• PySpark : count() √† nouveau\n",
    "# =====================================================================\n",
    "ln_aft = per_order_active.count()\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# üêº Pandas :\n",
    "# display(Markdown(f\"Apr√®s jointure+filtre actifs : **{len_init} ‚Üí {ln_aft}**\"))\n",
    "# display(per_order.head())\n",
    "# üî• PySpark :\n",
    "# - on garde le m√™me affichage Markdown\n",
    "# - pour l‚Äôaper√ßu, on utilise limit().toPandas()\n",
    "# =====================================================================\n",
    "display(Markdown(f\"Apr√®s jointure+filtre actifs : **{len_init} ‚Üí {ln_aft}**\"))\n",
    "\n",
    "per_order_active.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "739db941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+----------+\n",
      "|order_id     |created_at         |order_date|\n",
      "+-------------+-------------------+----------+\n",
      "|O202503010001|2025-03-01 11:30:49|2025-03-01|\n",
      "|O202503010003|2025-03-01 07:27:00|2025-03-01|\n",
      "|O202503010005|2025-03-01 22:29:42|2025-03-01|\n",
      "|O202503010007|2025-03-01 15:50:48|2025-03-01|\n",
      "|O202503010008|2025-03-01 20:56:15|2025-03-01|\n",
      "+-------------+-------------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================\n",
    "# üêº Pandas version :\n",
    "# def to_date(s):\n",
    "#     s = str(s)\n",
    "#     for fmt in (\"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%d\"):\n",
    "#         try:\n",
    "#             return datetime.strptime(s, fmt).date().isoformat()\n",
    "#         except ValueError:\n",
    "#             continue\n",
    "#     raise ValueError(\"Format de date non reconnu\")\n",
    "#\n",
    "# per_order[\"order_date\"] = per_order[\"created_at\"].apply(to_date)\n",
    "#\n",
    "# üî• PySpark version :\n",
    "# Pas d'apply ‚Üí on utilise les fonctions de parsing SQL :\n",
    "#   - to_timestamp()\n",
    "#   - to_date()\n",
    "#   - coalesce() pour tester plusieurs formats\n",
    "# ==========================================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Essayer plusieurs formats pour parser created_at\n",
    "order_date_col = F.to_date(\n",
    "    F.to_timestamp(F.col(\"created_at\"), \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "# 2Ô∏è‚É£ Deuxi√®me format si le premier √©choue\n",
    "order_date_alt = F.to_date(\n",
    "    F.to_timestamp(F.col(\"created_at\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Fusionner les deux tentatives (comme ton try/except en Python)\n",
    "order_date_final = F.coalesce(order_date_col, order_date_alt)\n",
    "\n",
    "# 4Ô∏è‚É£ Ajouter la colonne order_date au DataFrame Spark\n",
    "per_order_with_date = per_order_active.withColumn(\"order_date\", order_date_final)\n",
    "\n",
    "# 5Ô∏è‚É£ Aper√ßu (√©quivalent Pandas : df[[\"order_id\",\"created_at\",\"order_date\"]].head())\n",
    "per_order_with_date.select(\"order_id\", \"created_at\", \"order_date\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "323061d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Aper√ßu `per_order` avec refunds"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-------+-------------------+----------+-----------------+---------+---------+----------+-----------+\n",
      "|order_id     |customer_id|channel|created_at         |items_sold|gross_revenue_eur|city     |is_active|order_date|refunds_eur|\n",
      "+-------------+-----------+-------+-------------------+----------+-----------------+---------+---------+----------+-----------+\n",
      "|O202503010001|C0676      |web    |2025-03-01 11:30:49|5         |62.5             |Marseille|true     |2025-03-01|0.0        |\n",
      "|O202503010003|C0642      |web    |2025-03-01 07:27:00|1         |5.0              |Toulouse |true     |2025-03-01|-3.13      |\n",
      "|O202503010005|C0571      |web    |2025-03-01 22:29:42|1         |2.5              |Toulouse |true     |2025-03-01|-35.42     |\n",
      "|O202503010007|C0464      |app    |2025-03-01 15:50:48|1         |24.9             |Nantes   |true     |2025-03-01|0.0        |\n",
      "|O202503010008|C0317      |app    |2025-03-01 20:56:15|2         |30.0             |Marseille|true     |2025-03-01|-3.69      |\n",
      "+-------------+-----------+-------+-------------------+----------+-----------------+---------+---------+----------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================\n",
    "# üêº Pandas :\n",
    "# refunds_sum = refunds.groupby(\"order_id\", as_index=False)[\"amount\"].sum()\n",
    "# refunds_sum = refunds_sum.rename(columns={\"amount\":\"refunds_eur\"})\n",
    "#\n",
    "# üî• PySpark :\n",
    "# groupBy('order_id').agg(sum(\"amount_num\").alias(\"refunds_eur\"))\n",
    "#\n",
    "# ‚ö†Ô∏è On suppose que tu avais d√©j√† converti amount ‚Üí amount_num (double)\n",
    "# ==========================================================================\n",
    "\n",
    "refunds_sum_sdf = (\n",
    "    refunds_clean\n",
    "    .groupBy(\"order_id\")\n",
    "    .agg(F.sum(\"amount\").alias(\"refunds_eur\"))\n",
    ")\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "# üêº Pandas :\n",
    "# per_order = per_order.merge(refunds_sum, on=\"order_id\", how=\"left\")\n",
    "#\n",
    "# üî• PySpark :\n",
    "# per_order_sdf.join(refunds_sum_sdf, on=\"order_id\", how=\"left\")\n",
    "#\n",
    "# Spark join = merge Pandas\n",
    "# ==========================================================================\n",
    "\n",
    "per_order_refunded = (\n",
    "    per_order_with_date\n",
    "    .join(refunds_sum_sdf, on=\"order_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "# üêº Pandas : .fillna({\"refunds_eur\":0.0})\n",
    "#\n",
    "# üî• PySpark : fillna({\"refunds_eur\": 0.0})\n",
    "# ==========================================================================\n",
    "\n",
    "per_order_refunded = per_order_refunded.fillna({\"refunds_eur\": 0.0})\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "# üêº Pandas : display(per_order.head())\n",
    "#\n",
    "# üî• PySpark : limit().toPandas()\n",
    "# ==========================================================================\n",
    "\n",
    "display(Markdown(\"Aper√ßu `per_order` avec refunds\"))\n",
    "per_order_refunded.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "118c1e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "‚úîÔ∏è Table `orders_clean` sauvegard√©e dans SQLite depuis PySpark"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================================================\n",
    "# üêº Pandas :\n",
    "# per_order_save = per_order[[\"order_id\", ...]].copy()\n",
    "#\n",
    "# üî• PySpark :\n",
    "# select() pour choisir les colonnes\n",
    "# ==========================================================================\n",
    "\n",
    "per_order_save_sdf = per_order_refunded.select(\n",
    "    \"order_id\",\n",
    "    \"customer_id\",\n",
    "    \"city\",\n",
    "    \"channel\",\n",
    "    \"order_date\",\n",
    "    \"items_sold\",\n",
    "    \"gross_revenue_eur\"\n",
    ")\n",
    "\n",
    "# ==========================================================================\n",
    "# üêº Pandas : to_sql(...)\n",
    "#\n",
    "# üî• PySpark :\n",
    "# ‚Üí Spark ne peut PAS √©crire dans SQLite directement\n",
    "# ‚Üí Donc on convertit en pandas pour r√©utiliser to_sql()\n",
    "#\n",
    "# ‚ö†Ô∏è ATTENTION :\n",
    "# - toPandas() charge tout en m√©moire\n",
    "# - Assure-toi que le DF final n‚Äôest pas gigantesque\n",
    "# ==========================================================================\n",
    "\n",
    "per_order_save_pdf = per_order_save_sdf.toPandas()  # conversion Spark ‚Üí pandas\n",
    "\n",
    "# connexion SQLite\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# enregistrement\n",
    "per_order_save_pdf.to_sql(\n",
    "    \"orders_clean\",\n",
    "    conn,\n",
    "    if_exists=\"replace\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "display(Markdown(\"‚úîÔ∏è Table `orders_clean` sauvegard√©e dans SQLite depuis PySpark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fc6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+------------+----------------+----------+-----------------+-----------+------------------+\n",
      "|date      |city    |channel|orders_count|unique_customers|items_sold|gross_revenue_eur|refunds_eur|net_revenue_eur   |\n",
      "+----------+--------+-------+------------+----------------+----------+-----------------+-----------+------------------+\n",
      "|2025-03-01|Bordeaux|app    |4           |4               |8         |136.8            |-25.55     |111.25000000000001|\n",
      "|2025-03-01|Bordeaux|web    |6           |6               |13        |195.2            |-39.73     |155.47            |\n",
      "|2025-03-01|Lille   |app    |4           |4               |13        |224.2            |-24.47     |199.73            |\n",
      "|2025-03-01|Lille   |web    |3           |3               |12        |117.5            |-14.23     |103.27            |\n",
      "|2025-03-01|Lyon    |app    |7           |7               |16        |159.5            |-10.86     |148.64            |\n",
      "+----------+--------+-------+------------+----------------+----------+-----------------+-----------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Taille: (491, 9)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================================================\n",
    "# üêº Pandas :\n",
    "# agg = per_order.groupby([\"order_date\",\"city\",\"channel\"], as_index=False).agg(\n",
    "#     orders_count=(\"order_id\",\"nunique\"),\n",
    "#     unique_customers=(\"customer_id\",\"nunique\"),\n",
    "#     items_sold=(\"items_sold\",\"sum\"),\n",
    "#     gross_revenue_eur=(\"gross_revenue_eur\",\"sum\"),\n",
    "#     refunds_eur=(\"refunds_eur\",\"sum\")\n",
    "# )\n",
    "#\n",
    "# üî• PySpark :\n",
    "# - groupBy(...).agg(...)\n",
    "# - F.countDistinct(col) = nunique en pandas\n",
    "# - F.sum(col) = sum\n",
    "# ==========================================================================\n",
    "\n",
    "agg_sdf = (\n",
    "    per_order_refunded\n",
    "    .groupBy(\"order_date\", \"city\", \"channel\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"order_id\").alias(\"orders_count\"),                      # nunique(order_id)\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\"),               # nunique(customer_id)\n",
    "        F.bround(F.sum(\"items_sold\"),2).alias(\"items_sold\"),                    # sum(items_sold)\n",
    "        F.bround(F.sum(\"gross_revenue_eur\"),2).alias(\"gross_revenue_eur\"),      # sum(gross_revenue_eur)\n",
    "        F.bround(F.sum(\"refunds_eur\"),2).alias(\"refunds_eur\")                   # sum(refunds_eur)\n",
    "    )\n",
    ")\n",
    "\n",
    "# ==========================================================================\n",
    "# üêº Pandas :\n",
    "# agg[\"net_revenue_eur\"] = agg[\"gross_revenue_eur\"] + agg[\"refunds_eur\"]\n",
    "#\n",
    "# üî• PySpark :\n",
    "# withColumn(\"col\", expr)\n",
    "# ==========================================================================\n",
    "\n",
    "agg_sdf = agg_sdf.withColumn(\n",
    "    \"net_revenue_eur\",\n",
    "    F.col(\"gross_revenue_eur\") + F.col(\"refunds_eur\")\n",
    ")\n",
    "\n",
    "# ==========================================================================\n",
    "# üêº Pandas :\n",
    "# agg = agg.rename(columns={\"order_date\": \"date\"})\n",
    "#\n",
    "# üî• PySpark :\n",
    "# withColumnRenamed(\"old\", \"new\")\n",
    "# ==========================================================================\n",
    "\n",
    "agg_sdf = agg_sdf.withColumnRenamed(\"order_date\", \"date\")\n",
    "\n",
    "# ==========================================================================\n",
    "# üêº Pandas :\n",
    "# .sort_values([\"date\",\"city\",\"channel\"]).reset_index(drop=True)\n",
    "#\n",
    "# üî• PySpark :\n",
    "# - .orderBy() pour trier\n",
    "# - pas d'index ‚Üí rien √† reset\n",
    "# ==========================================================================\n",
    "\n",
    "agg_sdf = agg_sdf.orderBy(\"date\", \"city\", \"channel\")\n",
    "\n",
    "# ==========================================================================\n",
    "# üêº Pandas :\n",
    "# display(agg.head())\n",
    "# display(df.shape)\n",
    "#\n",
    "# üî• PySpark :\n",
    "# - limit().toPandas() pour afficher\n",
    "# - count() + len(columns)\n",
    "# ==========================================================================\n",
    "\n",
    "agg_sdf.show(5, truncate=False)\n",
    "\n",
    "# Taille\n",
    "rows = agg_sdf.count()\n",
    "cols = len(agg_sdf.columns)\n",
    "display(Markdown(f\"Taille: ({rows}, {cols})\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7a81a6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "‚úîÔ∏è Table `daily_city_sales` √©crite dans SQLite"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚úîÔ∏è Exports CSV √©crits dans `/mnt/c/Users/alexa/Simplon/Esther/Exos/Starter stack pour Data Engineers - Partie 1/data/out`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# On part de :\n",
    "#   - agg_sdf : DataFrame Spark agr√©g√© (date, city, channel, m√©triques)\n",
    "#   - db_path : chemin SQLite (Path)\n",
    "#   - out_dir : dossier de sortie (Path)\n",
    "#   - sep, enc, ffmt : param√®tres CSV\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# 1) Spark ‚Üí pandas (petit volume, OK pour la RAM)\n",
    "agg_pdf = agg_sdf.toPandas()\n",
    "\n",
    "# 2) √âcriture dans SQLite via pandas.to_sql\n",
    "conn = sqlite3.connect(db_path)\n",
    "agg_pdf.to_sql(\"daily_city_sales\", conn, if_exists=\"replace\", index=False)\n",
    "conn.close()\n",
    "display(Markdown(\"‚úîÔ∏è Table `daily_city_sales` √©crite dans SQLite\"))\n",
    "\n",
    "# 3) Exports CSV par date (un fichier par jour)\n",
    "for d, sub in agg_pdf.groupby(\"date\"):\n",
    "    # d est un objet date/Timestamp ‚Üí on formate proprement en YYYYMMDD\n",
    "    if hasattr(d, \"strftime\"):\n",
    "        d_str = d.strftime(\"%Y%m%d\")\n",
    "    else:\n",
    "        d_str = str(d).replace(\"-\", \"\")  # fallback si jamais\n",
    "\n",
    "    out_path = out_dir / f\"daily_summary_{d_str}.csv\"\n",
    "    sub[\n",
    "        [\n",
    "            \"date\",\n",
    "            \"city\",\n",
    "            \"channel\",\n",
    "            \"orders_count\",\n",
    "            \"unique_customers\",\n",
    "            \"items_sold\",\n",
    "            \"gross_revenue_eur\",\n",
    "            \"refunds_eur\",\n",
    "            \"net_revenue_eur\",\n",
    "        ]\n",
    "    ].to_csv(\n",
    "        out_path,\n",
    "        index=False,\n",
    "        sep=sep,\n",
    "        encoding=enc,\n",
    "        float_format=ffmt,\n",
    "    )\n",
    "\n",
    "# 4) Export CSV global (toutes les dates)\n",
    "all_path = out_dir / \"daily_summary_all.csv\"\n",
    "agg_pdf.to_csv(\n",
    "    all_path,\n",
    "    index=False,\n",
    "    sep=sep,\n",
    "    encoding=enc,\n",
    "    float_format=ffmt,\n",
    ")\n",
    "\n",
    "display(Markdown(f\"‚úîÔ∏è Exports CSV √©crits dans `{out_dir}`\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starter-stack-pour-data-engineers-partie-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
